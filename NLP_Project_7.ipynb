{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atlantic-brave",
   "metadata": {},
   "source": [
    "# Going Deeper(NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-basic",
   "metadata": {},
   "source": [
    "# 7. BERT pretrained model 제작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-worry",
   "metadata": {},
   "source": [
    "# mini BERT 만들기"
   ]
  },
  
  {
   "cell_type": "markdown",
   "id": "bridal-modem",
   "metadata": {},
   "source": [
    "## 목차\n",
    "    \n",
    "    Step 1. 서론\n",
    "    \n",
    "    Step 2. 토크나이저 생성\n",
    "\n",
    "    Step 3. Pretrain 데이터 생성\n",
    "    \n",
    "    Step 4. BERT 모델 생성\n",
    "\n",
    "    Step 5. BERT 모델 학습\n",
    "    \n",
    "    Step 6. BERT 모델 평가\n",
    "    \n",
    "    Step 7. 결론\n"
   ]
  },
  
  {
   "cell_type": "markdown",
   "id": "conscious-draft",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. 서론\n",
    "***\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; 본 예제에서는 한글 위키 문서를 이용하여 BERT 모델을 Pretrain 학습하고 모델의 성능을 평가 합니다. 한글 위키 문서는 다양한 주제에 대한 3,957,761개의 문단으로 이루어진 데이터 입니다. 한글 위키 문서를 바탕으로 SentencePiece 토크나이저를 생성하고 모델을 Pretrain 하기 위한 Masked Language Model(MLM)과 Next Sentence Prediction(NSP) 학습 데이터를 생성하여 모델을 학습, 평가 합니다. 다음은 예제의 진행 순서를 제시한 것입니다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-camcorder",
   "metadata": {},
   "source": [
    "#### 필요 라이브러리 및 패키지 호출\n",
    "***\n",
    "+ 예제에서 사용할 라이브러리 및 패키지를 호출 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "desirable-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os   #디렉토리 관리\n",
    "import re   #정규식\n",
    "import json   #파일\n",
    "import math   #수치 연산\n",
    "import random   #난수\n",
    "import numpy as np   #행렬 연산\n",
    "import matplotlib.pyplot as plt   #데이터 시각화\n",
    "from tqdm.notebook import tqdm   #과정 출력\n",
    "\n",
    "\n",
    "import sentencepiece as spm   #토크나이저\n",
    "import tensorflow as tf   #신경망\n",
    "import tensorflow.keras.backend as K   #신경망\n",
    "\n",
    "\n",
    "#난수 설정==================\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "#End========================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-volunteer",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 한글 위키 문서 출력\n",
    "***\n",
    "+ 예제에서는 한글 위키 문서를 바탕으로 BERT의 Pretrain을 위한 데이터를 생성 합니다.\n",
    "\n",
    "\n",
    "+ 한글 위키 문서는 총 3,957,761개의 문단으로 이루어져 있으며, 그 중 5개의 문단을 출력 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optional-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "지미 카터\n",
      "\n",
      "제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
      "\n",
      "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\n",
      "\n",
      "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
      "\n",
      "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.\n",
      "\n",
      "카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.\n",
      "\n",
      "====================================================================================================\n",
      "Line Num: 3,957,761\n"
     ]
    }
   ],
   "source": [
    "corpus_file = './dataset/kowiki.txt'\n",
    "\n",
    "f = open(corpus_file, 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "print(\"=\" * 100)\n",
    "for idx, line in enumerate(lines):\n",
    "    print(line.strip(), end=\"\\n\\n\")\n",
    "    if idx == 5: break;\n",
    "f.close()\n",
    "print(\"=\" * 100)\n",
    "print(f\"Line Num: {len(lines):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-redhead",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. 토크나이저 생성\n",
    "***\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; 한글 위키 문서 데이터를 바탕으로 SentencePiece 기반 토크나이저를 생성 합니다. 단어사전 크기는 8,000으로 설정 합니다. 이와 함께 특수 토큰 7개를 추가하여 총 8,007개의 토큰을 등록 합니다. [SEP], [CLS], [MASK] 토큰의 경우 BERT 모델의 Pretrain에 사용하는 토큰으로 순서대로 문장의 연결 구분, 문맥의 자연스러움, 단어 마스크에 사용 합니다. 최종 생성된 토크나이저는 재사용을 위해 저장 합니다.\n",
    "</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-machinery",
   "metadata": {},
   "source": [
    "#### SentencePiece 모델 생성\n",
    "***\n",
    "+ 한글 위키 문서 말뭉치 데이터를 바탕으로 8,007개(7개의 특수 토큰 포함)의 토큰을 가지는 SentencePiece 모델을 생성 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inside-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-thumbnail",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### SentencePiece 모델 불러오기\n",
    "***\n",
    "+ 앞서 생성한 SentencePiece 모델을 불러옵니다.\n",
    "\n",
    "\n",
    "+ 단어사전 크기는 특수토큰을 포함하여 8,007개 입니다.\n",
    "\n",
    "\n",
    "+ SentencePiece 토큰 5개를 출력 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "separate-revision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Vocab Size: 8,007\n",
      "SentencePiece Token: ['▁1', '▁이', '으로', '에서', '▁있']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_dir = './dataset/'\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}ko_8000.model\")\n",
    "\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"Vocab Size: {vocab.get_piece_size():,}\")\n",
    "print(\"SentencePiece Token:\", [vocab.id_to_piece(idx) for idx in range(7, 12)])\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-liberia",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Pretrain 데이터 생성\n",
    "***\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; BERT는 사전학습된 모델을 사용하는 것에 의의가 있습니다. BERT는 'Masked Language Model(MLM)'과 'Next Sentence Prediction(NSP)' 문제를 풀도록 학습합니다. MLM은 단어를 중심으로 학습하고 NSP는 문장을 중심으로 학습합니다. \n",
    "</span><br><br>\n",
    "\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; MLM은 문장의 특정 단어를 '[MASK]'로 바꾸고 이를 입력하여 가려진 단어를 맞추도록 학습합니다. BERT의 출력 행렬에서 MASK 위치의 벡터를 단어사전 크기의 softmax에 입력하여 확률분포 중 가장 높은 단어를 선택합니다. MLM은 단어에 MASK 처리 뿐만 아니라 특정 단어를 다른 단어로 바꾸어 입력한 후, 원래 단어는 무엇이었는지 맞추도록 하는 방법 또한 사용하였습니다.\n",
    "</span><br><br>\n",
    "\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; NSP는 두 문장을 연결하여 함께 입력합니다. 두 문장은 원래 연결되어있던 문장과 랜덤하게 연결한 문장을 입력하는데, 모델은 해당 문장이 실제로 연결되어있던 것인지에 대한 여부를 학습합니다. 이는 궁극적으로, '나는 사과를 좋아해.', '귤도 좋아해' 처럼 문장이 자연스럽게 이어지는 지에 대한 여부를 학습하게 되는 것입니다. 두 문장은 [SEP] 토큰을 통해 연결하여 입력합니다. 또한, [CLS] 토큰은 NSP의 분류를 위한 토큰으로 BERT의 출력에서 해당 위치의 벡터를 이진 분류 레이어에 입력하여 문장 연결의 자연스러운 정도를 출력합니다.\n",
    "</span><br><br>\n",
    "\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; BERT는 MLM과 NSP를 각각 진행하지 않고 동시에 시행 하였습니다. 다시 정리하자면, MLP는 특정 단어에 마스크를 씌우거나 다른 단어로 변경하여 이를 맞추도록 하는 '단어 중심'의 학습 이고 NSP는 실제 연결된 문장과 랜덤하게 연결한 문장을 입력하여 그 여부를 맞추도록 하는 '문장 중심'의 학습에 해당합니다.\n",
    "</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-association",
   "metadata": {},
   "source": [
    "#### MASK 생성 함수 정의\n",
    "***\n",
    "+ BERT Pretrain 방식 중 하나인 `Mask 생성`은 문장의 특정 부분을 가리고 BERT에게 해당 부분을 맞추도록 학습 합니다.\n",
    "\n",
    "\n",
    "+ 문장을 입력하면 랜덤하게 Mask를 생성하는 함수를 정의 합니다.\n",
    "\n",
    "\n",
    "+ `나는 어제 밥을 먹었다.` 문장을 입력하여 MASK를 생성하는 예제를 제시 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerical-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Example Sentence: ['▁나', '는', '▁어', '제', '▁', '밥', '을', '▁먹', '었다', '.']\n",
      "\n",
      "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ AFER MASKING ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
      "\n",
      "Masked Sentence: ['▁나', '는', '[MASK]', '[MASK]', '▁', '밥', '을', '▁먹', '었다', '.']\n",
      "`mask_idx`: [2, 3]\n",
      "`mask_label`: ['▁어', '제']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Mask 생성 함수=========================\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label\n",
    "#End====================================\n",
    "\n",
    "\n",
    "#Mask 생성 예제=========================\n",
    "tokens_org = vocab.encode_as_pieces(\"나는 어제 밥을 먹었다.\")\n",
    "\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.5)\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens_org.copy(), mask_cnt, vocab_list)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"Example Sentence:\", tokens_org, end=\"\\n\\n\")\n",
    "print(\" AFER MASKING \".center(50, \"↓\"), end=\"\\n\\n\")\n",
    "print(\"Masked Sentence:\", tokens)\n",
    "print(\"`mask_idx`:\", mask_idx)\n",
    "print(\"`mask_label`:\", mask_label)\n",
    "print(\"=\" * 100)\n",
    "#End===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-herald",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### NSP 생성 함수 정의\n",
    "***\n",
    "+ Next Sentence Prediction(NSP)는 BERT Pretrain 학습 방식 중 하나로, 두 문장을 연결하여 BERT에 입력하였을 때, 문장이 인위적으로 연결된 것인지 또는 기존 연결 되어 있던 문장인지에 대한 여부를 학습하여 모델이 문장의 문맥을 이해하도록 합니다.\n",
    "\n",
    "\n",
    "+ 말뭉치를 입력하면 NSP 데이터를 생성하는 함수를 정의 합니다.\n",
    "\n",
    "\n",
    "+ 앞서 정의한 Mask 생성 함수를 이용하여 Masking과 NSP를 동시에 진행 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "embedded-coral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "tokens: ['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]']\n",
      "\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "is_next: 1\n",
      "\n",
      "mask_idx: [13, 14, 15]\n",
      "\n",
      "mask_label: ['▁그', '날', '은']\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#토큰 길이 제한 함수============================\n",
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "#End============================================\n",
    "\n",
    "\n",
    "#NSP 데이터 생성 함수===========================\n",
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances\n",
    "#End============================================\n",
    "\n",
    "\n",
    "#NSP 생성 예제==================================\n",
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "\"\"\"\n",
    "\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "instances = create_pretrain_instances(vocab, doc, 64, 0.15, vocab_list)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "for key, value in instances[0].items():\n",
    "    print(f\"{key}: {value}\", end=\"\\n\\n\")\n",
    "print(\"=\" * 100)\n",
    "#End============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-flower",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Pretrain 데이터 생성\n",
    "***\n",
    "+ 앞서 생성한 Masking 함수와 NSP 함수를 이용하여 Pretrain 학습 데이터를 Json 형태로 생성 합니다.\n",
    "\n",
    "\n",
    "+ 총 918,189개의 데이터를 획득 하였습니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "refined-parks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Data Num: 918,189\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#create pretrain data=============================\n",
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []\n",
    "#End==============================================\n",
    "                \n",
    "\n",
    "#create data and save=============================\n",
    "pretrain_json_path = './dataset/bert_pre_train.json'\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)\n",
    "\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "        \n",
    "print(\"=\" * 100)        \n",
    "print(f\"Data Num: {total:,}\")\n",
    "print(\"=\" * 100)\n",
    "#End=============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-boring",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Pretrain 데이터 불러오기\n",
    "***\n",
    "+ Json 형태로 저장된 Pretrain 데이터를 불러 옵니다.\n",
    "\n",
    "\n",
    "+ 총 918,189개의 모든 데이터를 이용할 시, 1회 학습에 약 1시간 30분이 소요 되기 때문에, 총 데이터의 ¼ 수준의 114,773개의 학습 데이터를 불러 옵니다.\n",
    "\n",
    "\n",
    "+ np.memmap를 이용하여 메모리 효율을 도모 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amended-indicator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88fffd3b029490a8bf797ed6654e6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 114773 114773\n"
     ]
    }
   ],
   "source": [
    "#data load 함수=============================\n",
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)\n",
    "#End========================================\n",
    "\n",
    "\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=114773)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-indian",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 학습 데이터, 테스트 데이터 분할\n",
    "***\n",
    "+ 불러온 데이터를 학습에 사용할 데이터와 모델 평가에 사용할 테스트 데이터로 분할 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seeing-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = (pre_train_inputs[0][:100], pre_train_inputs[1][:100])\n",
    "test_label = (pre_train_labels[0][:100], pre_train_labels[1][:100])\n",
    "\n",
    "pre_train_inputs = (pre_train_inputs[0][100:], pre_train_inputs[1][100:])\n",
    "pre_train_labels = (pre_train_labels[0][100:], pre_train_labels[1][100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-passing",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. BERT 모델 생성\n",
    "***\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; BERT는 트랜스포머의 인코더 부분만 차용한 모델 입니다. BERT를 이해하기 위해서는 'RNN', 'seq2seq2', 'Attention', 'Transformer'에 대한 선행 지식이 요구 됩니다. BERT의 특징은 '선행 학습(Pretrain)된 모델을 개발자가 해결하고자 하는 문제에 맞게 모델을 조정(finetuning)하여 이용할 수 있다는 것입니다. 본 예제에서는 BERT의 선행 학습(Pretrain)을 수행 합니다. BERT의 작동 방식과 구조에 대한 설명은 'aiffel_exploration' 저장소의Exploration 17에서 자세히 다루고 있습니다. 예제에서 사용하는 BERT 모델은 Pretrain에서 수행하는 MLM, NSP 문제를 학습 하도록 설계 합니다. 모델의 파라미터 크기는 약 1M이 되도록 합니다.\n",
    "</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-hindu",
   "metadata": {},
   "source": [
    "#### BERT 모델 정의\n",
    "***\n",
    "+ BERT 모델을 생성하기 위한 함수를 정의 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "following-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유틸리티 함수++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
    "\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#데이터 관리+++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#임베딩 레이어+++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#DotProduct++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#멀티 헤드 어텐션++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(\n",
    "            config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    "        self.W_K = tf.keras.layers.Dense(\n",
    "            config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    "        self.W_V = tf.keras.layers.Dense(\n",
    "            config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(\n",
    "            config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(\n",
    "            tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3]\n",
    "        )  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(\n",
    "            tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3]\n",
    "        )  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(\n",
    "            tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3]\n",
    "        )  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#FFNN++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(\n",
    "            config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    "        self.W_2 = tf.keras.layers.Dense(\n",
    "            config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#인코더++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#BERT++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "#PositionalEmbedding+++++++++++++++++++++++++++++++++++++++++++\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "#End+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-cooking",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Pretrain을 위한 BERT 모델 정의\n",
    "***\n",
    "+ Pretrain을 학습하기 위한 BERT 모델 구성을 정의 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "minus-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의==================\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(\n",
    "            config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    "        self.dense2 = tf.keras.layers.Dense(\n",
    "            n_output, use_bias=False,\n",
    "            activation=tf.nn.softmax, name=\"nsp\",\n",
    "            kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer()\n",
    "        )\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "#End========================================\n",
    "\n",
    "\n",
    "#Pretrain BERT 모델 생성====================\n",
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "#End========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-paragraph",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Pretrain을 위한 BERT 모델 생성\n",
    "***\n",
    "+ 앞서 정의한 함수를 바탕으로 BERT 모델을 생성 합니다.\n",
    "\n",
    "\n",
    "+ 모델의 파라미터 크기는 10,804,736 입니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contrary-consultation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 512), (None, 10541056    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            263680      bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 10,804,736\n",
      "Trainable params: 10,804,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"d_model\": 512, \"n_head\": 8, \"d_head\": 64,\n",
    "    \"dropout\": 0.1, \"d_ff\": 1024,\n",
    "    \"layernorm_epsilon\": 0.001, \"n_layer\": 3,\n",
    "    \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0\n",
    "})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-blackberry",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. BERT 모델 학습\n",
    "***\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; 모델 옵티마이저를 설정하고 학습을 진행 합니다. 옵티마이저는 Adam을 사용하고, 학습률의 경우 고정된 값을 이용하지 않고 Consine Schedule을 이용하여, 학습 초반에는 높은 학습률로 설정한 후, 점차 낮추는 방식을 사용 합니다. 배치 사이즈는 64로 설정하고 학습 회수는 10회 진행 하였습니다. 그 결과, 최종 손실값은 NSP 문제에 대해 0.547, MLM 문제에 대해 9.885 였으며, 정확도는 NSP 문제에 대해 0.349, MLM 문제에 대해 0.320임을 확인 하였습니다.\n",
    "</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-chair",
   "metadata": {},
   "source": [
    "#### 학습 옵티마이저 설정\n",
    "***\n",
    "+ 학습률의 경우 고정된 값을 이용하지 않고 `CosineSchedule`를 이용하여, 높은 학습률로부터 점차 낮추는 방식을 사용 합니다.\n",
    "\n",
    "\n",
    "+ 옵티마이저는 Adam을 이용합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sunrise-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LearningRateScheduler================\n",
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "#End=================================\n",
    "\n",
    "#손실 함수===========================\n",
    "def lm_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "#End=================================\n",
    "\n",
    "\n",
    "#손실 함수===========================\n",
    "def lm_acc(y_true, y_pred):\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "#End=================================\n",
    "\n",
    "\n",
    "#옵티마이저 설정=====================\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(\n",
    "    loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss),\n",
    "    optimizer=optimizer,\n",
    "    metrics={\"nsp\": \"acc\", \"mlm\": lm_acc}\n",
    ")\n",
    "#End================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-library",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 모델 학습\n",
    "***\n",
    "+ 학습률의 경우 고정된 값을 이용하지 않고 `CosineSchedule`를 이용하여, 높은 학습률로부터 점차 낮추는 방식을 사용 합니다.\n",
    "\n",
    "\n",
    "+ 배치 사이즈는 64이며, 총 학습 회수는 10회로 설정 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "guilty-newcastle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1792/1792 [==============================] - 807s 447ms/step - loss: 20.1473 - nsp_loss: 0.6583 - mlm_loss: 19.4890 - nsp_acc: 0.5826 - mlm_lm_acc: 0.1169\n",
      "Epoch 2/10\n",
      "1792/1792 [==============================] - 802s 448ms/step - loss: 16.7802 - nsp_loss: 0.6244 - mlm_loss: 16.1558 - nsp_acc: 0.6129 - mlm_lm_acc: 0.1413\n",
      "Epoch 3/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 13.7717 - nsp_loss: 0.6156 - mlm_loss: 13.1561 - nsp_acc: 0.6169 - mlm_lm_acc: 0.2021\n",
      "Epoch 4/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 12.7131 - nsp_loss: 0.6087 - mlm_loss: 12.1044 - nsp_acc: 0.6292 - mlm_lm_acc: 0.2350\n",
      "Epoch 5/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 12.0500 - nsp_loss: 0.6038 - mlm_loss: 11.4462 - nsp_acc: 0.6424 - mlm_lm_acc: 0.2589\n",
      "Epoch 6/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 11.4691 - nsp_loss: 0.5968 - mlm_loss: 10.8723 - nsp_acc: 0.6541 - mlm_lm_acc: 0.2796\n",
      "Epoch 7/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 11.0613 - nsp_loss: 0.5853 - mlm_loss: 10.4760 - nsp_acc: 0.6789 - mlm_lm_acc: 0.2956\n",
      "Epoch 8/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 10.7472 - nsp_loss: 0.5714 - mlm_loss: 10.1758 - nsp_acc: 0.7029 - mlm_lm_acc: 0.3073\n",
      "Epoch 9/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 10.5355 - nsp_loss: 0.5579 - mlm_loss: 9.9777 - nsp_acc: 0.7253 - mlm_lm_acc: 0.3161\n",
      "Epoch 10/10\n",
      "1792/1792 [==============================] - 803s 448ms/step - loss: 10.4326 - nsp_loss: 0.5471 - mlm_loss: 9.8854 - nsp_acc: 0.7386 - mlm_lm_acc: 0.3198\n"
     ]
    }
   ],
   "source": [
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-market",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 학습에 따른 손실값 및 정확도 시각화\n",
    "***\n",
    "+ 10 epoch 동안의 손실값과 정확도를 시각화 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sized-collective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-testament",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 6. BERT 모델 평가\n",
    "***\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; 앞서 분할한 100개의 테스트 데이터를 이용하여 모델의 평가를 실시 합니다. 임의의 테스트 데이터 5개를 추출하여 모델에 입력하여 MLM과 NSP에 대한 예측을 확인하였습니다. 그 결과, MLM에 대해서는 대부분의 MASK를 올바르게 예측하지 못하였습니다. 이는 모델의 크기가 1M으로 작은 축에 속하기 때문으로 예상 됩니다. 또, MLM 문제는 총 8,000개의 단어 중 한 개를 예측해야 하므로 NSP 문제에 비해 난이도가 높습니다. NSP의 경우에는 총 5개 중 5개 모두 올바르게 예측 하였습니다. 100개의 테스트 데이터에 대한 모델의 성능을 확인하였습니다. MLM 손실값은 11.743이고 NSP 손실값은 0.677 이며, MLM 정확도는 0.281, NSP 정확도는 0.560 입니다.\n",
    "</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-evanescence",
   "metadata": {},
   "source": [
    "#### 테스트 데이터에 대한 모델의 예측 샘플\n",
    "***\n",
    "+ 임의의 5개 테스트 데이터를 모델에 입력하여, 모델의 예측을 확인 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "organic-platinum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================< DATA >==============================================\n",
      "Input Sentence: [CLS]의 와동에서 formula_8의 밀도에 따라에도 독립爾兢 순[MASK][MASK]풋폼[MASK][MASK][MASK] 보았다. 맥스웰은 와동의 밀도를 측정하기 위한 값으로 투자율 μ 을 정의하였다. 이 논문에서 밝힌 맥스웰의 개념은 다음과 같다.[SEP] 이 때 formula_14 는 전하 밀도이다. formula_8는 축을 이루어 회전하는 자기 전류이고 formula_9는 그 주위를 돌게 되는 자기력선의 70 선속이다. 투자율[MASK][MASK][MASK] 결국 자기장 formula_8에 의해[MASK][MASK][MASK][SEP]\n",
      "\n",
      "NSP Answer:   True\n",
      "[MASK] Answer: ['formula_9의', '와동', '운동이', '결정된다고', '자기', 'μ는', '유도되는']\n",
      "\n",
      "NSP Predict:  1\n",
      "[MASK] Predict: ['formula', '독립시.', '을', '변에', '밀율으로', '한', 'formula도는', 'formula_8']\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "==============================================< DATA >==============================================\n",
      "Input Sentence: [CLS] 제5공화국을 거치면서도 민주화운동을 계속하다가 1984년에는 민주통일 국민회의 고문을 지냈다. 1985년[MASK][MASK][MASK] 국민운동본부 고문이 되었다.[SEP] 그는 국가주의와 민족주의에 반대하였다. 한 인터뷰에서 그는 '민족통합을 참으로影 국민ST 우리의 대적이 누군가부터 분명히 알아야 합니다. 우리를 분열시킨 도둑이 누구입니까?[MASK][MASK][MASK][MASK] 소련?[MASK][MASK][MASK] 아닙니다.[MASK] 다른 민족이나 이데올로기 때문이 뛰어난침 자신이 합병 국민을 종[SEP]\n",
      "\n",
      "NSP Answer:   True\n",
      "[MASK] Answer: ['민주쟁취', '하려면', '일본?', '미국?', '중공?', '어느', '아닙니다.']\n",
      "\n",
      "NSP Predict:  1\n",
      "[MASK] Predict: ['민주월일', '했다,', '구만이', '언만', '또', '뛰어난침', '자신이', '새로운']\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "==============================================< DATA >==============================================\n",
      "Input Sentence: [CLS]-方程式, )은 전기와 자기의 발생, 전기장과 자기장,[MASK][MASK] 밀도와[MASK][MASK] 밀도의 형성을 나타내는 4개의 편미분 방정식이다. 맥스웰 방정식은 빛 역시 전자기파의[MASK][MASK] 보여준다. 각각의 방정식은 가우스 법칙, 가우스 자기 법칙,[MASK][MASK][MASK][MASK][MASK][MASK] 유도 법칙, 앙페르 회로 법칙으로 불린다. 각각의 있으며勸텅 제임스 클러크 맥스웰이 종합한 이후 맥스웰 방정식으로 불리게 되었다.[SEP] 맥스웰 방정식[SEP]\n",
      "\n",
      "NSP Answer:   False\n",
      "[MASK] Answer: ['전하', '전류', '방정식은', '하나임을', '패러데이', '전자기', '방정식을']\n",
      "\n",
      "NSP Predict:  0\n",
      "[MASK] Predict: ['가,', '가우스', '방정식은', '변성을', '맥스스스정,', '방勸텅']\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "==============================================< DATA >==============================================\n",
      "Input Sentence: [CLS] 수가 아닌 수 중에서 처음으로 초월수임이 증명된 수는 상수 e로, 샤를[MASK][MASK][MASK][MASK] 1873년에쳄자와弐 1882년에는 페르디난트 폰 린데만이 원주율[MASK] 초월수임을[MASK][MASK][MASK][MASK] 고대 그리스 시대부터의 난제였던 원적문제가 불가능함을 보여주는 결과였다.[SEP] 초월수의 존재는 레온하르트 오일러가 예상하였으나, 최초의 초월수는 1844년에 조제프 리우빌이 발견하였다. 그는 초월수의 예로서 다음과 같이[MASK][MASK] 리우빌 상수를[MASK][MASK][MASK][SEP]\n",
      "\n",
      "NSP Answer:   False\n",
      "[MASK] Answer: ['에르미트가', '증명하였다.', '또한', '증명하였다.', '이것은', '최초의', '정의되는', '제시하였다.']\n",
      "\n",
      "NSP Predict:  0\n",
      "[MASK] Predict: ['수하였다.', '바하였다.', '시대에', '증명시하였다,', '최초의', '같다.', '증명하였다.']\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "==============================================< DATA >==============================================\n",
      "Input Sentence: [CLS]훈(鄭相勳), 유석동(柳錫東), 양인성(楊仁性)등과 함께 교회에[MASK][MASK][MASK] 않고도 신앙을 유지하는[MASK][MASK][MASK] 신앙클럽을 결성하였다. 1927년 동인지 《성서조선 聖書朝鮮》 창간에 참여하고 논객으로 글을 발표하기 시작하였다.[SEP] 1923년 오산학교를 졸업하고, 1924년 일본 동경고등사범학교 문과 1부에 입학하여, 우치무라 간조의 성서 집회에 참가하여[MASK] 무교회주의를 접했다.[SEP]\n",
      "\n",
      "NSP Answer:   False\n",
      "[MASK] Answer: ['다니지', '유지하는', '무교회주의', '시작하였다.', '동경고등사범학교', '그의']\n",
      "\n",
      "NSP Predict:  0\n",
      "[MASK] Predict: ['참여지하지', '유지하는', '데인과', '시작하였다.', '동경고등사범학교', '일본']\n",
      "====================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _idx in np.random.randint(100, size=5).tolist():\n",
    "    print(\"< DATA >\".center(100, \"=\"))\n",
    "    print(\"Input Sentence:\", vocab.decode_ids(test_input[0][_idx].tolist()), end=\"\\n\\n\")\n",
    "    print(\"NSP Answer:\".ljust(13), bool(test_label[0][_idx]))\n",
    "    print(\"[MASK] Answer:\".ljust(13), vocab.decode_ids(test_label[1][_idx].tolist()).split(), end=\"\\n\\n\")\n",
    "\n",
    "    nsp_predict, mask_predict = pre_train_model((test_input[0][_idx].reshape(1, -1), test_input[1][_idx].reshape(1, -1)))\n",
    "\n",
    "    mask_predict = [np.argmax(token) for token in mask_predict[0]]\n",
    "    mask_predict *= np.clip(test_label[1][_idx], 0, 1)\n",
    "\n",
    "    print(\"NSP Predict:\".ljust(13), np.argmax(nsp_predict))\n",
    "    print(\"[MASK] Predict:\".ljust(13), vocab.decode_ids(mask_predict.tolist()).split())\n",
    "    print(\"=\" * 100, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-desktop",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 테스트 데이터 평가\n",
    "***\n",
    "+ 테스트 데이터 100개에 대한 모델의 성능을 확인 합니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "identified-davis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 69ms/step - loss: 12.4192 - nsp_loss: 0.6767 - mlm_loss: 11.7425 - nsp_acc: 0.5600 - mlm_lm_acc: 0.2810\n"
     ]
    }
   ],
   "source": [
    "_ = pre_train_model.evaluate(test_input, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-truth",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 7. 결론\n",
    "***\n",
    "<span style=\"font-size:12pt; line-height:1.7; font-family:Serif;\">\n",
    "    &nbsp; &nbsp; 한글 위키 문서를 이용하여 BERT 모델을 Pretrain하고 평가하였습니다. BERT를 Pretrain 하기위해 데이터를 MLM과 NSP 문제에 맞게 전처리 하고 이를 학습 하였습니다. 데이터는 학습 데이터 114,673개와 테스트 데이터 100개 입니다. 단어사전에 총 8,007개의 토큰을 등록하였습니다. BERT 모델의 파라미터 크기는 약 1M으로 설정하였습니다. 테스트 데이터에 대한 학습된 모델 성능을 확인한 결과, MLM 손실값은 11.743이고 NSP 손실값은 0.677 이며, MLM 정확도는 0.281, NSP 정확도는 0.560 입니다. [표 1]은 BERT 모델의 테스트 데이터에 대한 손실값 및 정확도를 제시한 것입니다.\n",
    "</span>\n",
    "\n",
    "\n",
    "|-|MLM|NSP|Total|\n",
    "|:--------:|:--------:|:--------:|:--------:|\n",
    "|**Loss**|11.743|0.677|12.419|\n",
    "|**Accuracy**|0.281|0.560|-|\n",
    "\n",
    "[표 1] BERT 모델의 테스트 데이터에 대한 손실값 및 정확도\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
