# NLP_Going_Deeper
Project with NLP topic

### 이 저장소(Repository)는 「NLP Going Deeper 과정」에 대한 자료를 다루고 있습니다.

***
작성자: MRSYS

최종 수정일: 2022-02-03

+ 학습 기간: 2021-12-14 ~ 2022-01-23

<br>

|순번|폴더|제목|학습 내용|비고|
|:--------:|:--------:|:--------:|:--------:|:--------:|
|1|[going deeper_02](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_1.ipynb)|2. 단어사전 생성법(2021-12-14)|자연어 토큰화 방법에 따른 모델 성능을 비교합니다.토큰화 기법은 '공백 기반 토큰화', '형태소 기반 토큰화', 'SentencePiece 기반 토큰화'를 사용하였습니다.|-|
|2|[going deeper_04](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_2.ipynb)|4. 뉴스 카테고리 다중분류(2021-12-17)|머신러닝 모델과 딥러닝 모델의 성능 차이를 확인하고 단어사전 크기에 따른 모델의 성능 차이를 로이터 뉴스 데이터셋을 이용하여 확인하였습니다. |-|
|3|[going deeper_06](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_3.ipynb)|6. 임베딩 내 편향성 (2021-12-20)| 21개의 영화 장르에 따른 예술 영화와 일반 영화에 대한 편향성을 WEAT를 이용하여 측정 하였습니다.|-|
|4|[going deeper_08](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_4.ipynb)|8. Seq2seq으로 번역기 (2021-12-27)|Bahdanau Attention(바다나우 어텐션)을 이용하여 한국어-영어 번역 모델을 생성하고 이를 평가 하였습니다.|-|
|5|[going deeper_10](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_5.ipynb)|10. Transformer로 번역기 (2021-12-31)| 한영 번역 모델을 Transformer를 이용하여 만들고 결과를 보았습니다. 또한, Bahadanau Attention과 Transformer 모델의 결과 비교와 Mecab과 SentencePiece를 이용한 토큰화 기법에 따른 번역 결과를 비교 하였습니다.|-|
|6|[going deeper_12](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_6.ipynb)|12. 번역 대화 능력(2022-01-13)|한국어 챗봇 모델을 Transformer 이용하여 생성하고 BLEU를 이용하여 모델을 평가하였습니다. |-|
|7|[going deeper_14](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_7.ipynb)|14. BERT model 제작(2022-01-18)|3,957,761개의 문단으로 이루어진 한글 위키 문서를 이용하여 BERT 모델을 Pretrain하고 평가하였습니다.|-|
|8|[going deeper_16](https://github.com/mrsys/NLP_Going_Deeper/blob/main/NLP_Project_8.ipynb)|16. HuggingFace 커스텀 프로젝트(2022-01-23)|BERT, RoBERTa 모델과 토크나이저를 Hugging Face의 transformers를 이용하여 불러와 MNLI 데이터를 학습하고 각 모델의 성능을 확인하였습니다. |-|
